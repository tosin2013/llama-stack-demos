{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef1ba28",
   "metadata": {},
   "source": [
    "# Advanced Multi-Agent Orchestration with A2A and Llama Stack\n",
    "\n",
    "This notebook demonstrates how to construct and orchestrate a multi-agent system using the Agent-to-Agent (A2A) communication protocol. We will build individual agents using Llama Stack and then enable them to collaborate on complex tasks by exposing their functionalities via A2A servers. \n",
    "\n",
    "This demo focuses on an orchestration pattern where a planner agent determines which specialized agent (skill) to call, and a composer agent formats the final response.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1.  **Setting up the Llama Stack Environment**: Initializing the Llama Stack client and configuring model parameters.\n",
    "\n",
    "2.  **Defining Llama Stack Agents**: Creating three distinct Llama Stack agents:\n",
    "\n",
    "    * `Planner Agent`: Responsible for interpreting user queries and creating a plan to use other agents' skills.\n",
    "\n",
    "    * `Custom Tool Agent`: Equipped with tools for random number generation and date retrieval.\n",
    "\n",
    "    * `Composer Agent`: Skilled at generating human-friendly text from structured data.\n",
    "\n",
    "3.  **Serving Llama Stack Agents via A2A**: Exposing each Llama Stack agent over an individual A2A server, making their `AgentCard` skills accessible via the A2A protocol.\n",
    "\n",
    "4.  **Orchestrating the A2A Agents**: Setting up an `AgentManager` to manage communication with the A2A-enabled agents and implementing an `orchestrate` function to coordinate them.\n",
    "\n",
    "5.  **Running the Orchestration**: Launching the multi-agent system to answer user queries, leveraging the planner to select tools/skills and the composer to generate a final, human-readable response.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "- `python_requires >= 3.13`\n",
    "\n",
    "- Followed the instructions in the [Setup Guide](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb) notebook.\n",
    "\n",
    "## Additional environment variables\n",
    "This demo requires the following environment variables in addition to those defined in the [Setup Guide](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb):\n",
    "\n",
    "- `PLANNER_AGENT_LOCAL_PORT`: The port for the A2A agent responsible for planning (e.g. 10020).\n",
    "\n",
    "- `CUSTOM_TOOL_AGENT_LOCAL_PORT`: The port for the A2A agent with custom tool capabilities (e.g. 10021).\n",
    "\n",
    "- `COMPOSER_AGENT_LOCAL_PORT`: The port for the A2A agent responsible for composing final answers (e.g. 10022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c88e09",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "To provide A2A communication capabilities, we will use the [sample implementation by Google](https://github.com/google/A2A/tree/main/samples/python). Please make sure that the content of the referenced directory is available on your Python path. This can be done, for example, by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c195db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'a2a-samples' already exists and is not an empty directory.\n",
      "Collecting annotated-types==0.7.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting anyio==4.9.0 (from -r ../requirements.txt (line 2))\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: appnope==0.1.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 4)) (3.0.0)\n",
      "Collecting certifi==2025.1.31 (from -r ../requirements.txt (line 5))\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting cffi==1.17.1 (from -r ../requirements.txt (line 6))\n",
      "  Using cached cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting charset-normalizer==3.4.2 (from -r ../requirements.txt (line 7))\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting click==8.1.8 (from -r ../requirements.txt (line 8))\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm==0.2.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 9)) (0.2.2)\n",
      "Collecting cryptography==45.0.3 (from -r ../requirements.txt (line 10))\n",
      "  Using cached cryptography-45.0.3-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: debugpy==1.8.14 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 11)) (1.8.14)\n",
      "Requirement already satisfied: decorator==5.2.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 12)) (5.2.1)\n",
      "Collecting distro==1.9.0 (from -r ../requirements.txt (line 13))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dotenv==0.9.9 (from -r ../requirements.txt (line 14))\n",
      "  Using cached dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: executing==2.2.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 15)) (2.2.0)\n",
      "Collecting fire==0.7.0 (from -r ../requirements.txt (line 16))\n",
      "  Using cached fire-0.7.0-py3-none-any.whl\n",
      "Collecting h11==0.16.0 (from -r ../requirements.txt (line 17))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting httpcore==1.0.9 (from -r ../requirements.txt (line 18))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting httpx==0.28.1 (from -r ../requirements.txt (line 19))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx-sse==0.4.0 (from -r ../requirements.txt (line 20))\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting idna==3.10 (from -r ../requirements.txt (line 21))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 22)) (6.29.5)\n",
      "Requirement already satisfied: ipython==9.3.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 23)) (9.3.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 24)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 25)) (0.19.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 26)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.8.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 27)) (5.8.1)\n",
      "Collecting jwcrypto==1.5.6 (from -r ../requirements.txt (line 28))\n",
      "  Using cached jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting llama_stack_client==0.2.2 (from -r ../requirements.txt (line 29))\n",
      "  Using cached llama_stack_client-0.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r ../requirements.txt (line 30))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 31)) (0.1.7)\n",
      "Collecting mdurl==0.1.2 (from -r ../requirements.txt (line 32))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 33)) (1.6.0)\n",
      "Collecting numpy==2.2.5 (from -r ../requirements.txt (line 34))\n",
      "  Using cached numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging==25.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 35)) (25.0)\n",
      "Collecting pandas==2.2.3 (from -r ../requirements.txt (line 36))\n",
      "  Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: parso==0.8.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 37)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 38)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.3.8 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 39)) (4.3.8)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 40)) (3.0.51)\n",
      "Requirement already satisfied: psutil==7.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 41)) (7.0.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 42)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 43)) (0.2.3)\n",
      "Collecting pyaml==25.1.0 (from -r ../requirements.txt (line 44))\n",
      "  Using cached pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pycparser==2.22 (from -r ../requirements.txt (line 45))\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting pydantic==2.11.3 (from -r ../requirements.txt (line 46))\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pydantic_core==2.33.1 (from -r ../requirements.txt (line 47))\n",
      "  Using cached pydantic_core-2.33.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 48)) (2.19.1)\n",
      "Collecting PyJWT==2.10.1 (from -r ../requirements.txt (line 49))\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 50)) (2.9.0.post0)\n",
      "Collecting python-dotenv==1.1.0 (from -r ../requirements.txt (line 51))\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pytz==2025.2 (from -r ../requirements.txt (line 52))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting PyYAML==6.0.2 (from -r ../requirements.txt (line 53))\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pyzmq==26.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 54)) (26.4.0)\n",
      "Collecting requests==2.32.3 (from -r ../requirements.txt (line 55))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich==14.0.0 (from -r ../requirements.txt (line 56))\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six==1.17.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 57)) (1.17.0)\n",
      "Collecting sniffio==1.3.1 (from -r ../requirements.txt (line 58))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting sse-starlette==2.2.1 (from -r ../requirements.txt (line 59))\n",
      "  Using cached sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 60)) (0.6.3)\n",
      "Collecting starlette==0.46.2 (from -r ../requirements.txt (line 61))\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting termcolor==3.0.1 (from -r ../requirements.txt (line 62))\n",
      "  Using cached termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: tornado==6.5.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 63)) (6.5.1)\n",
      "Collecting tqdm==4.67.1 (from -r ../requirements.txt (line 64))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 65)) (5.14.3)\n",
      "Collecting typing-inspection==0.4.0 (from -r ../requirements.txt (line 66))\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting typing_extensions==4.13.2 (from -r ../requirements.txt (line 67))\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting tzdata==2025.2 (from -r ../requirements.txt (line 68))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting urllib3==2.4.0 (from -r ../requirements.txt (line 69))\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting uvicorn==0.34.2 (from -r ../requirements.txt (line 70))\n",
      "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /Users/kcogan/Documents/llama-stack-on-ocp/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 71)) (0.2.13)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached cryptography-45.0.3-cp311-abi3-macosx_10_9_universal2.whl (7.1 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
      "Using cached llama_stack_client-0.2.2-py3-none-any.whl (273 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.1-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing_extensions, tqdm, termcolor, sniffio, PyYAML, python-dotenv, PyJWT, pycparser, numpy, mdurl, idna, httpx-sse, h11, distro, click, charset-normalizer, certifi, annotated-types, uvicorn, typing-inspection, requests, pydantic_core, pyaml, pandas, markdown-it-py, httpcore, fire, dotenv, cffi, anyio, starlette, rich, pydantic, httpx, cryptography, sse-starlette, llama_stack_client, jwcrypto\n",
      "Successfully installed PyJWT-2.10.1 PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.2 click-8.1.8 cryptography-45.0.3 distro-1.9.0 dotenv-0.9.9 fire-0.7.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.0 idna-3.10 jwcrypto-1.5.6 llama_stack_client-0.2.2 markdown-it-py-3.0.0 mdurl-0.1.2 numpy-2.2.5 pandas-2.2.3 pyaml-25.1.0 pycparser-2.22 pydantic-2.11.3 pydantic_core-2.33.1 python-dotenv-1.1.0 pytz-2025.2 requests-2.32.3 rich-14.0.0 sniffio-1.3.1 sse-starlette-2.2.1 starlette-0.46.2 termcolor-3.0.1 tqdm-4.67.1 typing-inspection-0.4.0 typing_extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 uvicorn-0.34.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/google-a2a/a2a-samples.git\n",
    "! pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192c7e7",
   "metadata": {},
   "source": [
    "Now, we will add the paths to the A2A library and our own tools to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a99e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# the path of the A2A library\n",
    "sys.path.append('./a2a-samples/samples/python')\n",
    "# the path to our own utils\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be7200",
   "metadata": {},
   "source": [
    "We will now proceed with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26570e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.server import A2AServer\n",
    "from common.types import AgentCard, AgentSkill, AgentCapabilities\n",
    "from common.client import A2AClient, A2ACardResolver\n",
    "from common.utils.push_notification_auth import PushNotificationReceiverAuth\n",
    "from hosts.cli.push_notification_listener import PushNotificationListener\n",
    "\n",
    "from a2a_llama_stack.A2ATool import A2ATool\n",
    "from a2a_llama_stack.task_manager import AgentTaskManager\n",
    "\n",
    "# for asynchronously serving the A2A agent\n",
    "import threading\n",
    "\n",
    "\n",
    "import json\n",
    "import urllib.parse\n",
    "from uuid import uuid4\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f018c",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de3ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: llama3.1:8b-instruct-fp16\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# agent related imports\n",
    "import uuid\n",
    "from llama_stack_client import Agent\n",
    "# from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6631ae7",
   "metadata": {},
   "source": [
    "## 2. Setting Up and Serving A2A Agents\n",
    "Now, we will define the core Llama Stack agents that will form our multi-agent system. These agents are created using the Llama Stack `Agent` class. Later, we will expose their functionalities via A2A servers.\n",
    "\n",
    "We will initialize three distinct Llama Stack agents:\n",
    "\n",
    "1. **Planner Agent**: an agent that acts as an orchestrator, determining which skills (other agents) are needed to answer a user's query.\n",
    "\n",
    "2. **Custom Tool Agent**: an agent equipped with tools to generate random numbers and provide the current date.\n",
    "\n",
    "3. **Composer Agent**: an agent skilled at writing human-friendly text based on provided information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57e65d",
   "metadata": {},
   "source": [
    "#### 2.1. Planner Agent\n",
    "The Planner Agent is responsible for understanding the user's query and determining which skills (exposed by other agents) are needed to fulfill the request. It outputs a plan, typically a list of skill IDs to be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64c0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=(\"You are an orchestration assistant. Ensure you count correctly the number of skills needed.\"),\n",
    "    sampling_params=sampling_params,\n",
    "    tools=[],\n",
    "    max_infer_iters=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb66f6a",
   "metadata": {},
   "source": [
    "#### 2.2. Custom Tool Agent\n",
    "First, we define the Python functions that will serve as custom tools for our second agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972b1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def random_number_tool() -> int:\n",
    "    \"\"\"\n",
    "    Generate a random integer between 1 and 100.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\nGenerating a random number...\\n\\n\")\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "\n",
    "def date_tool() -> str:\n",
    "    \"\"\"\n",
    "    Return today's date in YYYY-MM-DD format.\n",
    "    \"\"\"\n",
    "    return datetime.utcnow().date().isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934c92d",
   "metadata": {},
   "source": [
    "Next, initialize the Llama Stack agent, providing it with these tools and instructions on how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6bef0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tool_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=(\n",
    "            \"You have access to two tools:\\n\"\n",
    "            \"- random_number_tool: generates one random integer between 1 and 100\\n\"\n",
    "            \"- date_tool: returns today's date in YYYY-MM-DD format\\n\"\n",
    "            \"Always use the appropriate tool to answer user queries.\"\n",
    "        ),    \n",
    "    sampling_params=sampling_params,\n",
    "    tools=[random_number_tool, date_tool],\n",
    "    max_infer_iters=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc992470",
   "metadata": {},
   "source": [
    "#### 2.3. Composer Agent\n",
    "Finally, we initialize the Composer Agent. This agent's role is to take structured data (e.g. results from other tools or agents) and formulate a coherent, human-friendly response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b71d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=(\"You are skilled at writing human-friendly text based on the query and associated skills.\"),   \n",
    "    sampling_params=sampling_params,\n",
    "    tools=[],\n",
    "    max_infer_iters=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba981a33",
   "metadata": {},
   "source": [
    "## 3. Serving Llama Stack Agents via A2A\n",
    "Now that we have our Llama Stack agents, we need to make their functionalities accessible via the A2A protocol. This involves:\n",
    "\n",
    "- Creating an `AgentCard`: An object containing metadata about the agent, including its URL and exposed capabilities (`AgentSkill`).\n",
    "\n",
    "- Wrapping the Llama Stack agent with an `AgentTaskManager`: An adapter that allows the A2A server to forward requests to the Llama Stack agent.\n",
    "\n",
    "- Creating and launching an `A2AServer`: A REST API server that handles A2A protocol communication for this agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d78d6",
   "metadata": {},
   "source": [
    "#### 3.1. Serving the Planner Agent\n",
    "First, we serve the Planner Agent via its own A2A server. Its `AgentCard` will highlight its orchestration planning skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c7e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [85772]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 48] error while attempting to bind on address ('::1', 10020, 0, 0): [errno 48] address already in use\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "planner_agent_local_port = int(os.getenv(\"planner_agent_LOCAL_PORT\", \"10020\"))\n",
    "planner_agent_url = f\"http://localhost:{planner_agent_local_port}\"\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"Orchestration Agent\",\n",
    "    description=\"Plans which tool to call for each user question\",\n",
    "    url=planner_agent_url,\n",
    "    version=\"0.1.0\",\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"text/plain\"],\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=False,\n",
    "        pushNotifications=False,\n",
    "        stateTransitionHistory=False,\n",
    "        ),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"orchestrate\",\n",
    "            name=\"Orchestration Planner\",\n",
    "            description=\"Plan user questions into JSON steps of {skill_id}\",\n",
    "            tags=[\"orchestration\"],\n",
    "            examples=[\"Plan: What's today's date and a random number?\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"application/json\"],\n",
    "            ),\n",
    "    ],\n",
    ")\n",
    "task_manager = AgentTaskManager(agent=planner_agent)\n",
    "server = A2AServer(\n",
    "    agent_card=agent_card,\n",
    "    task_manager=task_manager,\n",
    "    host='localhost',\n",
    "    port=planner_agent_local_port\n",
    ")\n",
    "thread = threading.Thread(target=server.start, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3292fc9",
   "metadata": {},
   "source": [
    "#### 3.2. Serving the Custom Tool Agent\n",
    "We create an `AgentCard` for the Custom Tool Agent, detailing its skills (random number generation and date retrieval). Then, we wrap our `custom_tool_agent` (the Llama Stack Agent) in an `AgentTaskManager` and start an A2AServer for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab56668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [85772]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 48] error while attempting to bind on address ('127.0.0.1', 10021): [errno 48] address already in use\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "custom_tool_agent_local_port = int(os.getenv(\"CUSTOM_TOOL_AGENT_LOCAL_PORT\", \"10021\"))\n",
    "custom_tool_agent_url = f\"http://localhost:{custom_tool_agent_local_port}\"\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"Custom Agent\",\n",
    "    description=\"Generates random numbers or retrieve today's dates\",\n",
    "    url=custom_tool_agent_url,\n",
    "    version=\"0.1.0\",\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"text/plain\"],\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=False,\n",
    "        pushNotifications=False,\n",
    "        stateTransitionHistory=False,\n",
    "        ),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"random_number_tool\", \n",
    "            name=\"Random Number Generator\",\n",
    "            description=\"Generates a random number between 1 and 100\",\n",
    "            tags=[\"random\"],\n",
    "            examples=[\"Give me a random number between 1 and 100\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"text/plain\"],\n",
    "            ),\n",
    "        AgentSkill(\n",
    "            id=\"date_tool\",\n",
    "            name=\"Date Provider\",\n",
    "            description=\"Returns today's date in YYYY-MM-DD format\",\n",
    "            tags=[\"date\"],\n",
    "            examples=[\"What's the date today?\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"text/plain\"],\n",
    "            ),\n",
    "    ],\n",
    ")\n",
    "task_manager = AgentTaskManager(agent=custom_tool_agent)\n",
    "server = A2AServer(\n",
    "    agent_card=agent_card,\n",
    "    task_manager=task_manager,\n",
    "    host='localhost',\n",
    "    port=custom_tool_agent_local_port\n",
    ")\n",
    "thread = threading.Thread(target=server.start, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18ef4f",
   "metadata": {},
   "source": [
    "#### 3.3. Serving the Composer Agent\n",
    "Similarly, we set up an A2A server for the Composer Agent. Its `AgentCard` will define its writing skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee06174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [85772]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 48] error while attempting to bind on address ('::1', 10022, 0, 0): [errno 48] address already in use\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "composer_agent_local_port = int(os.getenv(\"COMPOSER_AGENT_LOCAL_PORT\", \"10022\"))\n",
    "composer_agent_url = f\"http://localhost:{composer_agent_local_port}\"\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"Writing Agent\",\n",
    "    description=\"Generate human-friendly text based on the query and associated skills\",\n",
    "    url=composer_agent_url,\n",
    "    version=\"0.1.0\",\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"text/plain\"],\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=False,\n",
    "        pushNotifications=False,\n",
    "        stateTransitionHistory=False,\n",
    "        ),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"writing_agent\", \n",
    "            name=\"Writing Agent\",\n",
    "            description=\"Write human-friendly text based on the query and associated skills\",\n",
    "            tags=[\"writing\"],\n",
    "            examples=[\"Write human-friendly text based on the query and associated skills\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"application/json\"],\n",
    "            ),\n",
    "    ],\n",
    ")\n",
    "task_manager = AgentTaskManager(agent=composer_agent)\n",
    "server = A2AServer(\n",
    "    agent_card=agent_card,\n",
    "    task_manager=task_manager,\n",
    "    host='localhost',\n",
    "    port=composer_agent_local_port\n",
    ")\n",
    "thread = threading.Thread(target=server.start, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b60a60",
   "metadata": {},
   "source": [
    "## 4. Orchestrating the A2A Agents\n",
    "With all Llama Stack agents defined and served via A2A, we now set up the client-side logic to interact with them. This involves managing connections and coordinating the flow of information between the Planner Agent, Skill Executor Agents (Custom Tool Agent), and the Composer Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75663ca",
   "metadata": {},
   "source": [
    "#### 4.1. Agent Manager\n",
    "The `AgentManager` class helps manage connections and agent cards for the orchestrator (Planner Agent) and the skill executor agents (Custom Tool Agent, Composer Agent). It simplifies accessing their A2A clients and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c328fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentInfo = Tuple[str, Any, A2AClient, str]\n",
    "\n",
    "class AgentManager:\n",
    "    def __init__(self, urls: List[str]):\n",
    "        # first URL is your orchestrator‚Ä¶\n",
    "        self.orchestrator: AgentInfo = self._make_agent_info(urls[0])\n",
    "        # ‚Ä¶the rest are skill agents, each keyed by skill.id\n",
    "        self.skills: Dict[str, AgentInfo] = {\n",
    "            skill.id: info\n",
    "            for url in urls[1:]\n",
    "            for info in (self._make_agent_info(url),)\n",
    "            for skill in info[1].skills\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_agent_info(url: str) -> AgentInfo:\n",
    "        card   = A2ACardResolver(url).get_agent_card()\n",
    "        client = A2AClient(agent_card=card)\n",
    "        session = uuid4().hex\n",
    "        return url, card, client, session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1260a5",
   "metadata": {},
   "source": [
    "#### 4.2. Orchestration Helper Functions\n",
    "These asynchronous helper functions are used by the main orchestration logic to send tasks to the A2A agents and process their responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f76784de",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _send_payload(client, card, session, payload, streaming: bool) -> str:\n",
    "    if not streaming:\n",
    "        res = await client.send_task(payload)\n",
    "        return res.result.status.message.parts[0].text.strip()\n",
    "\n",
    "    text = \"\"\n",
    "    async for ev in client.send_task_streaming(payload):\n",
    "        part = ev.result.status.message.parts[0].text or \"\"\n",
    "        print(part, end=\"\", flush=True)\n",
    "        text = part\n",
    "    print()\n",
    "    return text\n",
    "\n",
    "def _build_skill_meta(mgr):\n",
    "    \"\"\"Gather unique metadata for every skill in all executor cards.\"\"\"\n",
    "    unique_skills = {}  # Use a dictionary to store skills by their ID\n",
    "    for _, card, _, _ in mgr.skills.values():\n",
    "        for s in card.skills:\n",
    "            if s.id not in unique_skills:\n",
    "                unique_skills[s.id] = {\n",
    "                    \"skill_id\": s.id,\n",
    "                    \"name\": s.name,\n",
    "                    \"description\": getattr(s, \"description\", None),\n",
    "                    \"tags\": getattr(s, \"tags\", []),\n",
    "                    \"examples\": getattr(s, \"examples\", None),\n",
    "\n",
    "                }\n",
    "    return list(unique_skills.values()) # Convert the dictionary values back to a list\n",
    "\n",
    "\n",
    "async def _send_task(mgr, client, card, session, question, push=False, host=None, port=None) -> str:\n",
    "    \"\"\"Build a card-driven payload (with optional push) and dispatch it.\"\"\"\n",
    "    # Input parts\n",
    "    content = {\"question\": question}\n",
    "    modes   = getattr(card, \"acceptedInputModes\", [\"text\"])\n",
    "    parts   = ([{\"type\": \"json\", \"json\": content}]\n",
    "               if \"json\" in modes\n",
    "               else [{\"type\": \"text\", \"text\": json.dumps(content)}])\n",
    "\n",
    "    # Optional push URL & auth\n",
    "    can_push = push and getattr(card.capabilities, \"pushNotifications\", False)\n",
    "    push_url = (urllib.parse.urljoin(f\"http://{host}:{port}\", \"/notify\")\n",
    "                if can_push and host and port else None)\n",
    "    schemes = getattr(card.authentication, \"supportedSchemes\", [\"bearer\"])\n",
    "\n",
    "    # Assemble payload\n",
    "    payload = {\n",
    "        \"id\": uuid4().hex,\n",
    "        \"sessionId\": session,\n",
    "        \"acceptedOutputModes\": card.defaultOutputModes,\n",
    "        \"message\": {\"role\": \"user\", \"parts\": parts},\n",
    "        **({\"pushNotification\": {\"url\": push_url,\n",
    "                                 \"authentication\": {\"schemes\": schemes}}}\n",
    "           if push_url else {})\n",
    "    }\n",
    "\n",
    "    # Dispatch, letting the card decide streaming vs one-shot\n",
    "    stream = getattr(card.capabilities, \"streaming\", False)\n",
    "    return await _send_payload(client, card, session, payload, stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d27e5",
   "metadata": {},
   "source": [
    "#### 4.3. Orchestration Logic\n",
    "\n",
    "The `orchestrate` function coordinates the multi-agent interaction:\n",
    "\n",
    "1. **Planning Phase**: It queries the Planner Agent (via A2A) with the user's question and metadata about available skills from other agents. The Planner Agent returns a JSON plan (a list of `skill_id`s).\n",
    "\n",
    "2. **Execution Phase**: It iterates through the plan, calling the appropriate Skill Executor A2A Agents (e.g., Custom Tool Agent's skills) for each step.\n",
    "\n",
    "3. **Composition Phase**: Finally, it sends the original question and the collected results from the execution phase to the Composer A2A Agent to generate a polished, human-readable response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e5e4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def orchestrate(\n",
    "    agent_manager: AgentManager,\n",
    "    question: str,\n",
    "    push: bool = False,\n",
    "    push_receiver: str = \"http://localhost:5000\",\n",
    ") -> str:\n",
    "    # Unpack orchestrator info\n",
    "    orch_url, orch_card, orch_client, orch_session = agent_manager.orchestrator\n",
    "\n",
    "    # Optionally start push listener\n",
    "    host = port = None\n",
    "    if push:\n",
    "        parsed = urllib.parse.urlparse(push_receiver)\n",
    "        host, port = parsed.hostname, parsed.port\n",
    "        auth = PushNotificationReceiverAuth()\n",
    "        await auth.load_jwks(f\"{orch_url}/.well-known/jwks.json\")\n",
    "        PushNotificationListener(host, port, auth).start()\n",
    "\n",
    "    # --- Planning Phase ---\n",
    "    print(\"\\n\\033[1;33m=========== üß† Planning Phase ===========\\033[0m\")\n",
    "    # Build skill metadata\n",
    "    skills_meta = _build_skill_meta(agent_manager)\n",
    "    plan_instructions = (\n",
    "        \"You are an orchestration assistant.\\n\"\n",
    "        \"Available skills (id & name & description & tags & examples):\\n\"\n",
    "        f\"{json.dumps(skills_meta, indent=2)}\\n\\n\"\n",
    "        \"When given a user question, respond _only_ with a JSON array of objects, \"\n",
    "        \"each with key `skill_id`, without any surrounding object. You may be asked to write single or multiple skills.\\n\"\n",
    "        \"For example for multiple tools:\\n\"\n",
    "        \"[\"\n",
    "        \"{\\\"skill_id\\\": \\\"tool_1\\\"}, \"\n",
    "        \"{\\\"skill_id\\\": \\\"tool_2\\\"}\"\n",
    "        \"]\"\n",
    "    )\n",
    "    combined = plan_instructions + \"\\n\\nUser question: \" + question\n",
    "    raw = await _send_task(agent_manager, orch_client, orch_card, orch_session, combined, push=push, host=host, port=port)\n",
    "    print(f\"Raw plan ‚û°Ô∏è {raw}\")\n",
    "    try:\n",
    "        plan = json.loads(raw[: raw.rfind(\"]\") + 1])\n",
    "    except ValueError:\n",
    "        print(\"\\033[31mPlan parse failed, fixing invalid JSON...\\033[0m\")\n",
    "        fixer = \"Fix this json to be valid: \" + raw\n",
    "        fixed = await _send_task(agent_manager, orch_client, orch_card, orch_session, fixer, push=push, host=host, port=port)\n",
    "        plan = json.loads(fixed)\n",
    "    print(f\"\\n\\033[1;32mFinal plan ‚û°Ô∏è {plan}\\033[0m\")\n",
    "\n",
    "    # --- Execution Phase ---\n",
    "    print(\"\\n\\033[1;33m=========== ‚ö°Ô∏è Execution Phase ===========\\033[0m\")\n",
    "    parts = []\n",
    "    for i, step in enumerate(plan, 1):\n",
    "        sid = step[\"skill_id\"]\n",
    "        inp = json.dumps(step.get(\"input\", {}))\n",
    "        print(f\"‚û°Ô∏è Step {i}: {sid}({inp})\")\n",
    "\n",
    "        info = agent_manager.skills.get(sid)\n",
    "        if not info:\n",
    "            print(f\"\\033[31mNo executor for '{sid}', skipping.\\033[0m\")\n",
    "            parts.append({\"skill_id\": sid, \"output\": None})\n",
    "            continue\n",
    "\n",
    "        _, skill_card, skill_client, skill_sess = info\n",
    "        out = await _send_task(agent_manager, skill_client, skill_card, skill_sess, f\"{sid}({inp})\", push=push, host=host, port=port)\n",
    "        print(f\"   ‚úÖ ‚Üí {out}\")\n",
    "        parts.append({\"skill_id\": sid, \"output\": out})\n",
    "\n",
    "    # --- Composing Answer ---\n",
    "    print(\"\\n\\033[1;33m=========== üõ†Ô∏è Composing Answer ===========\\033[0m\")\n",
    "    comp_prompt = (\n",
    "        f\"Using the following information: {json.dumps(parts)}, \"\n",
    "        f\"write a clear and human-friendly response to the question: '{question}'. \"\n",
    "        \"Keep it concise and easy to understand and respond like a human with character. \"\n",
    "        \"Only use the information provided. If you cannot answer the question, say 'I don't know'. \"\n",
    "        \"Never show any code or JSON or Markdown, just the answer.\\n\\n\"\n",
    "    )\n",
    "    _, write_card, write_client, write_sess = agent_manager.skills[\"writing_agent\"]\n",
    "    final = await _send_task(agent_manager, write_client, write_card, write_sess, comp_prompt, push=push, host=host, port=port)\n",
    "\n",
    "    print(\"\\n\\033[1;36müéâ FINAL ANSWER\\033[0m\")\n",
    "    print(final)\n",
    "    print(\"\\033[1;36m====================================\\033[0m\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dbee6",
   "metadata": {},
   "source": [
    "### 5. Running the Orchestration\n",
    "Now we define the URLs for our orchestrator (Planner Agent) and skill executor agents (Custom Tool Agent, Composer Agent). We then initialize the `AgentManager` and call the `orchestrate` function with sample questions.\n",
    "\n",
    "The `AgentManager` uses the first URL for the orchestrator/planner and the rest for skill executors. The `orchestrate` function will then:\n",
    "\n",
    "1. Query the Planner Agent with the user's question and the list of available skills.\n",
    "\n",
    "2. The Planner Agent returns a plan (e.g. `[{'skill_id': 'random_number_tool'}]`).\n",
    "\n",
    "3. The `orchestrate` function executes the plan by calling the specified skill agents.\n",
    "\n",
    "4. Finally, it sends the original question and the skill outputs to the Composer Agent to generate a polished, human-readable response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19d1fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m===================== üõ∞Ô∏è Connected Agents =====================\u001b[0m\n",
      "Orchestrator: http://localhost:10020 (Orchestration Agent)\n",
      "Executors:\n",
      "  ‚Ä¢ random_number_tool -> http://localhost:10021 (Custom Agent)\n",
      "  ‚Ä¢ date_tool -> http://localhost:10021 (Custom Agent)\n",
      "  ‚Ä¢ writing_agent -> http://localhost:10022 (Writing Agent)\n",
      "\u001b[1;36m===============================================================\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== üß† Planning Phase ===========\u001b[0m\n",
      "Raw plan ‚û°Ô∏è [\n",
      "  {\"skill_id\": \"date_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"}\n",
      "]\n",
      "\n",
      "\u001b[1;32mFinal plan ‚û°Ô∏è [{'skill_id': 'date_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ‚ö°Ô∏è Execution Phase ===========\u001b[0m\n",
      "‚û°Ô∏è Step 1: date_tool({})\n",
      "   ‚úÖ ‚Üí {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"date_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-03\"{\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-03\"The date today is 2025-06-03.\n",
      "‚û°Ô∏è Step 2: random_number_tool({})\n",
      "   ‚úÖ ‚Üí {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:26{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:4{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "‚û°Ô∏è Step 3: random_number_tool({})\n",
      "   ‚úÖ ‚Üí {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:80{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:43```\n",
      "{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}} \n",
      "```\n",
      "‚û°Ô∏è Step 4: random_number_tool({})\n",
      "   ‚úÖ ‚Üí {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:88{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:16{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "‚û°Ô∏è Step 5: random_number_tool({})\n",
      "   ‚úÖ ‚Üí {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:81{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:48```\n",
      "{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "‚û°Ô∏è Step 6: random_number_tool({})\n",
      "   ‚úÖ ‚Üí {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:92{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:80{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "\n",
      "\u001b[1;33m=========== üõ†Ô∏è Composing Answer ===========\u001b[0m\n",
      "\n",
      "\u001b[1;36müéâ FINAL ANSWER\u001b[0m\n",
      "Today's date is June 3rd, 2025.\n",
      "\n",
      "Here are five random numbers for you: 26, 4, 80, 43, and 16.\n",
      "\u001b[1;36m====================================\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== üß† Planning Phase ===========\u001b[0m\n",
      "Raw plan ‚û°Ô∏è [{\"skill_id\": \"date_tool\"}]\n",
      "\n",
      "\u001b[1;32mFinal plan ‚û°Ô∏è [{'skill_id': 'date_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ‚ö°Ô∏è Execution Phase ===========\u001b[0m\n",
      "‚û°Ô∏è Step 1: date_tool({})\n",
      "   ‚úÖ ‚Üí {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"date_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-03\"{\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-03\"The date today is 2025-06-03.\n",
      "\n",
      "\u001b[1;33m=========== üõ†Ô∏è Composing Answer ===========\u001b[0m\n",
      "\n",
      "\u001b[1;36müéâ FINAL ANSWER\u001b[0m\n",
      "The current date is 2025-06-03.\n",
      "\u001b[1;36m====================================\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== üß† Planning Phase ===========\u001b[0m\n",
      "Raw plan ‚û°Ô∏è [\n",
      "  {\"skill_id\": \"random_number_tool\"}\n",
      "]\n",
      "\n",
      "\u001b[1;32mFinal plan ‚û°Ô∏è [{'skill_id': 'random_number_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ‚ö°Ô∏è Execution Phase ===========\u001b[0m\n",
      "‚û°Ô∏è Step 1: random_number_tool({})\n",
      "   ‚úÖ ‚Üí {\n",
      "    \"type\": \"function\",\n",
      "    \"name\": \"random_number_tool\",\n",
      "    \"parameters\": {}\n",
      "}Tool:random_number_tool Args:{}Tool:random_number_tool Response:38{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:85```\n",
      "{\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}\n",
      "\n",
      "\u001b[1;33m=========== üõ†Ô∏è Composing Answer ===========\u001b[0m\n",
      "\n",
      "\u001b[1;36müéâ FINAL ANSWER\u001b[0m\n",
      "Here's your randomly generated number: 38.\n",
      "\u001b[1;36m====================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ORCHESTRATOR_URL = \"http://localhost:10020\"\n",
    "EXECUTOR_URLS    = [\"http://localhost:10021\", \"http://localhost:10022\"]\n",
    "URLS             = [ORCHESTRATOR_URL, *EXECUTOR_URLS]\n",
    "\n",
    "_agent_manager = AgentManager(URLS)\n",
    "orch_url, orch_card, *_ = _agent_manager.orchestrator\n",
    "\n",
    "print(\"\\n\\033[1;36m===================== üõ∞Ô∏è Connected Agents =====================\\033[0m\")\n",
    "print(f\"Orchestrator: {orch_url} ({orch_card.name})\")\n",
    "print(\"Executors:\")\n",
    "for sid, (u, card, *_) in _agent_manager.skills.items():\n",
    "    print(f\"  ‚Ä¢ {sid} -> {u} ({card.name})\")\n",
    "print(\"\\033[1;36m===============================================================\\033[0m\")\n",
    "\n",
    "questions = [ \n",
    "    \"Get todays date then generate five random numbers\",\n",
    "    \"Get todays date?\",\n",
    "    \"I want one random number\",\n",
    "    ]\n",
    "\n",
    "for question in questions:\n",
    "    await orchestrate(_agent_manager, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d32564",
   "metadata": {},
   "source": [
    "## 6. Wrapping Up & Future Directions\n",
    "\n",
    "We've successfully orchestrated a team of specialized Llama Stack agents, showcasing how they can collaborate via the A2A protocol to tackle complex queries.\n",
    "\n",
    "**What We Achieved:**\n",
    "\n",
    "* We configured the Llama Stack environment and designed three distinct agents: a `Planner`, a `Custom Tool Agent` (with date/random number skills), and a `Composer`.\n",
    "\n",
    "* Each agent was made accessible through A2A, with an `AgentManager` and an `orchestrate` function guiding their interaction to deliver user-friendly answers.\n",
    "\n",
    "The key insight is the power of modular, specialized agents communicating through a standard protocol, allowing for flexible and scalable AI system development.\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "Inspired? Here are a few ways to build on this foundation:\n",
    "\n",
    "* **Refine & Expand**: Experiment with agent instructions or add new tools and specialized agents to the team.\n",
    "\n",
    "* **Boost Orchestration**: Explore more dynamic planning, such as conditional logic or inter-agent feedback loops.\n",
    "\n",
    "* **Challenge the System**: Test with increasingly complex queries to push the boundaries of the current setup.\n",
    "\n",
    "This notebook serves as a stepping stone into the exciting world of multi-agent AI. We hope it empowers you to build even more sophisticated applications. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
