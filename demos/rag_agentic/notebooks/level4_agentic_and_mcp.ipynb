{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ecd807-fb1c-41eb-a948-365e57396d90",
   "metadata": {},
   "source": [
    "# Level 4: Agentic & MCP (Medium Difficulty)\n",
    "\n",
    "This tutorial is aimed at those already familiar with basic Agentic workflows. It is meant to showcase  **sequential tool calls** or **conditional logic** within the context of an agentic workflow.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial we will be connecting to a llamastack instance, building an agent with various tools available to it, and inferencing against the agent.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "- Access to an openshift cluster.\n",
    "- A deployment of the [openshift MCP server](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/mcp-servers/openshift) within the openshift cluster (see the [deployment manifests](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/kubernetes/mcp-servers/openshift-mcp) for assistance with this).\n",
    "- User variables configured (e.g., inference_model, TAVILY_SEARCH_API_KEY, LLAMA_STACK_ENDPOINT).\n",
    "- A Tavily API key is required. You can register for one at https://tavily.com/.\n",
    "\n",
    "## General Setup - Agnostic to all Queries\n",
    "\n",
    "These steps will be the same for all 3 queries, with the exception of inputing the Tavily API key - this will only be used for query 2, and so could be ommitted if one only wants to run demos 1 or 3.\n",
    "\n",
    "### Configuring logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fc0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681412e1",
   "metadata": {},
   "source": [
    "### Connecting to llama-stack server\n",
    "\n",
    "For the llama-stack instance, you can either run it locally or connect to a remote llama-stack instance.\n",
    "\n",
    "#### Remote llama-stack\n",
    "\n",
    "- For remote, be sure to set `remote` to `True` and populate the `remote_llama_stack_endpoint` variable with your llama-stack remote.\n",
    "- [Remote Setup Guide](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/kubernetes)\n",
    "\n",
    "#### Local llama-stack\n",
    "- For local, be sure to set `remote` to `False` and validate the `local_llama_stack_endpoint` variable. It is based off of the default llama-stack port which is `8321` but is configurable with your deployment of llama-stack.\n",
    "- [Local Setup Guide](https://github.com/redhat-et/agent-frameworks/tree/main/prototype/frameworks/llamastack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa38ad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_stack_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m local_llama_stack_endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8321\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m tavily_search_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtavily-api-key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Replace with your Tavily API key (required for demo 2)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_stack_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaStackClient\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remote:\n\u001b[1;32m     12\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m remote_llama_stack_endpoint\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_stack_client'"
     ]
    }
   ],
   "source": [
    "remote = True # Use the `remote` variable to switching between a local development environment and a remote kubernetes cluster.\n",
    "model=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "remote_llama_stack_endpoint = \"your-llama-stack-endpoint\"  # Replace with your llama-stack endpoint if remote is set to True\n",
    "local_llama_stack_endpoint = \"http://localhost:8321\"\n",
    "\n",
    "tavily_search_api_key = \"tavily-api-key\" # Replace with your Tavily API key (required for demo 2)\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "if remote:\n",
    "    base_url = remote_llama_stack_endpoint\n",
    "else:\n",
    "    base_url = local_llama_stack_endpoint\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data={\n",
    "        \"tavily_search_api_key\": tavily_search_api_key # This is required for demo 2\n",
    "    })\n",
    "logger.info(f\"Connected to Llama Stack server @ {base_url} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66044170",
   "metadata": {},
   "source": [
    "### Validate tools are available in our llama-stack instance\n",
    "\n",
    "When an instance of llama-stack is redeployed your tools need to re-registered. Also if a tool is already registered with a llama-stack instance, if you try to register one with the same `toolgroup_id`, llama-stack will throw you an error.\n",
    "\n",
    "For this reason it is recommended to include some code to validate your tools and toolgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'NameError'>",
     "evalue": "name 'some_python_code_for_l4_q1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msome_python_code_for_l4_q1\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'some_python_code_for_l4_q1' is not defined"
     ]
    }
   ],
   "source": [
    "registered_tools = client.tools.list()\n",
    "registered_tools_identifiers = [t.identifier for t in registered_tools]\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "if  \"builtin::websearch\" not in registered_toolgroups: # Required for demo 2\n",
    "    error = AssertionError(\"Expected tool `builtin::websearch` to exist, but does not. Please fix your llama-stack deployment.\")\n",
    "    logger.error(error)\n",
    "    raise error\n",
    "    \n",
    "if \"mcp::openshift\" not in registered_toolgroups: # required for demos 1 and 3\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::openshift\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":mcp_url},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5cbe2",
   "metadata": {},
   "source": [
    "## Query 1: (Agentic) `Check the status of my OpenShift cluster. If it’s running, create a new pod named test-pod in the dev namespace.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d285f7d-09a5-47a2-ad92-938e0a8f0d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_python_code_for_l4_q1\n"
     ]
    }
   ],
   "source": [
    "print(\"some_python_code_for_l4_q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9830e45-5633-4eb3-9270-643a27e24f2a",
   "metadata": {},
   "source": [
    "### Query 2: (Agentic): `Search for the latest Red Hat OpenShift version on the Red Hat website. Summarize the version number and draft a short email to my team.`\n",
    "\n",
    "Previously, we instantiated our llama-stack client with our tavily search API key, and connected to our instance of that llama-stack client, before ensuring our required tools and toolgroups are registered to that llamastack instance.\n",
    "\n",
    "Now we can create our agent, start our agent sessions, ask our LLM the question, and print the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe55883a-6887-43dd-9498-5333a51799e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'ModuleNotFoundError'>",
     "evalue": "No module named 'llama_stack_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Code bellow written following examples here: https://llama-stack.readthedocs.io/en/latest/building_applications\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_stack_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_stack_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_logger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventLogger\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_stack_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaStackClient\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_stack_client'"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=client,\n",
    "    model=model,\n",
    "    instructions=\"\"\"You are a helpful AI assistant, responsible for helping me find and communicate information back to my team.\n",
    "    You have access to a number of tools.\n",
    "    Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "    When you are asked to search the web you must use a tool.\n",
    "    When signing off on emails, please be sure to include: - Sent from my llama-stack agent in the signature\n",
    "    \"\"\"\n",
    "    tools=[\"builtin::websearch\", \"mcp::openshift\"],\n",
    "    tool_config={\"tool_choice\":\"auto\"},\n",
    "    sampling_params={\n",
    "        \"max_tokens\":4096,\n",
    "        \"strategy\": {\"type\": \"greedy\"},\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "session_id = agent.create_session(session_name=\"Draft_email_with_latest_OCP_version\")\n",
    "prompt = \"\"\"Search for the web for the latest Red Hat OpenShift version on the Red Hat website. Summarize the version number and draft an email to convey this information.\"\"\"\n",
    "turn_response = agent.create_turn(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    session_id=session_id,\n",
    "    stream=True,\n",
    ")\n",
    "for log in EventLogger().log(turn_response):\n",
    "    log.print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa880bbc-bf69-4777-9417-ef7b13d51785",
   "metadata": {},
   "source": [
    "### Query 3: (MCP) `Review OpenShift logs for pods pod-123 and pod-456. Categorize each as ‘Normal’ or ‘Error’. If any show ‘Error’, send a Slack message to the ops team. Otherwise, show a simple summary.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7957e4-581c-4c3d-aee7-b8e3d9f2d0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_python_code_for_l4_q3\n"
     ]
    }
   ],
   "source": [
    "print(\"some_python_code_for_l4_q3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f56741-889c-4055-8c2a-e1fbcef9d720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
