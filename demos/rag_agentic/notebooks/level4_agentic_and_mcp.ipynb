{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b5ecd807-fb1c-41eb-a948-365e57396d90",
      "metadata": {},
      "source": [
        "# Level 4: Agentic & MCP (Medium Difficulty)\n",
        "\n",
        "This tutorial is aimed at those already familiar with basic Agentic workflows. It is meant to showcase  **sequential tool calls** or **conditional logic** within the context of an agentic workflow.\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this tutorial we will be connecting to a llama-stack instance, building an agent with various tools available to it, and inferencing against the agent.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have the following:\n",
        "- Access to an openshift cluster with A deployment of the [openshift MCP server](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/mcp-servers/openshift) (see the [deployment manifests](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/kubernetes/mcp-servers/openshift-mcp) for assistance with this).\n",
        "- User variables configured (see section `Setting your ENV variables` below).\n",
        "- A Tavily API key is required. You can register for one at https://tavily.com/home.\n",
        "\n",
        "## General Setup - Agnostic to all Queries\n",
        "\n",
        "These steps will be the same for all 3 queries, with the exception of inputing the Tavily API key - this will only be used for query 2, and so could be ommitted if one only wants to run demos 1 or 3.\n",
        "\n",
        "### Setting your ENV variables:\n",
        "\n",
        "As mentioned above, for this demo there are a few ENV variables that need to set:\n",
        "- `REMOTE` (boolean): dictates if you are using a remote llama-stack instance.\n",
        "- `REMOTE_BASE_URL` (string): the URL for your llama-stack instance if using remote connection.\n",
        "- `TAVILY_SEARCH_API_KEY` (string): your API key for tavily search. One can get one by going to: https://tavily.com/home.\n",
        "- `REMOTE_OCP_MCP_URL` (string): the URL for your Openshift MCP server. If the client does not find the tool registered to the llama-stack instance, it will use this URL to register the Openshift tool (used in demos 1 and 3).\n",
        "- `USE_PROMPT_CHAINING` (boolean): dictates if the prompt should be formatted as 3 separate prompts to isolate the steps. This is an option because a single inference call is able to do the search and draft the email, but by destructuring this into sub-steps where the LLM can focus on a single task at a time can theoretically increase performance. Additionally while this task can be accomplished from a single prompt, other more complex questions may not be doable in this way. This prompt-chaining example is meant to demonstrate how one might do this.\n",
        "\n",
        "### Installing dependencies\n",
        "\n",
        "This code requires `llama-stack` and the `llama-stack-client`, both at version `0.1.9`. Lets begin by installing them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4481ba68",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-stack-client==0.1.9 llama-stack==0.1.9"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ee3cb2",
      "metadata": {},
      "source": [
        "### Configuring logging\n",
        "\n",
        "Now that we have our dependencies, lets setup logging for the application:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fc0a44",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "if not logger.hasHandlers():  \n",
        "    logger.setLevel(logging.INFO)\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    stream_handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(message)s')\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    logger.addHandler(stream_handler)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "681412e1",
      "metadata": {},
      "source": [
        "### Connecting to llama-stack server\n",
        "\n",
        "For the llama-stack instance, you can either run it locally or connect to a remote llama-stack instance.\n",
        "\n",
        "#### Remote llama-stack\n",
        "\n",
        "- For remote, be sure to set `remote` to `True` and populate the `remote_llama_stack_endpoint` variable with your llama-stack remote.\n",
        "- [Remote Setup Guide](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/kubernetes)\n",
        "\n",
        "#### Local llama-stack\n",
        "- For local, be sure to set `remote` to `False` and validate the `local_llama_stack_endpoint` variable. It is based off of the default llama-stack port which is `8321` but is configurable with your deployment of llama-stack.\n",
        "- [Local Setup Guide](https://github.com/redhat-et/agent-frameworks/tree/main/prototype/frameworks/llamastack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa38ad0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "remote = os.getenv(\"REMOTE\") # Use the `remote` variable to switching between a local development environment and a remote kubernetes cluster.\n",
        "model=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "if remote and remote is True:\n",
        "    base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
        "else:\n",
        "    base_url = \"http://localhost:8321\"\n",
        "\n",
        "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\") # Replace with your Tavily API key (required for demo 2)\n",
        "\n",
        "from llama_stack_client import LlamaStackClient\n",
        "\n",
        "client = LlamaStackClient(\n",
        "    base_url=base_url,\n",
        "    provider_data={\n",
        "        \"tavily_search_api_key\": tavily_search_api_key # This is required for demo 2\n",
        "    })\n",
        "logger.info(f\"Connected to Llama Stack server @ {base_url} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66044170",
      "metadata": {},
      "source": [
        "### Validate tools are available in our llama-stack instance\n",
        "\n",
        "When an instance of llama-stack is redeployed your tools need to re-registered. Also if a tool is already registered with a llama-stack instance, if you try to register one with the same `toolgroup_id`, llama-stack will throw you an error.\n",
        "\n",
        "For this reason it is recommended to include some code to validate your tools and toolgroups. This is where the `mcp_url` comes into play. The following code will check that both the `builtin::websearch` and the `mcp::openshift` tools are registered as tools, but if the `mcp::openshift` tool is not listed there, it will attempt to register it using the mcp url.\n",
        "\n",
        "If you are running the MCP server from source, the default value for this is: `http://localhost:8000/sse`.\n",
        "\n",
        "If you are running the MCP server from a container, the default value for this is: `http://host.containers.internal:8000/sse`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
      "metadata": {},
      "outputs": [],
      "source": [
        "mcp_url = os.getenv(\"REMOTE_OCP_MCP_URL\") # Optional: enter your MCP server url here\n",
        "\n",
        "registered_tools = client.tools.list()\n",
        "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
        "if  \"builtin::websearch\" not in registered_toolgroups: # Required for demo 2\n",
        "    client.toolgroups.register(\n",
        "        toolgroup_id=\"builtin::websearch\",\n",
        "        provider_id=\"tavily-search\",\n",
        "        args={\"max_results\": 10},\n",
        "    )\n",
        "    \n",
        "if \"mcp::openshift\" not in registered_toolgroups: # required for demos 1 and 3\n",
        "    client.toolgroups.register(\n",
        "        toolgroup_id=\"mcp::openshift\",\n",
        "        provider_id=\"model-context-protocol\",\n",
        "        mcp_endpoint={\"uri\":mcp_url},\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef5cbe2",
      "metadata": {},
      "source": [
        "## Query 1: (Agentic) `Check the status of my OpenShift cluster. If it’s running, create a new pod named test-pod in the dev namespace.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d285f7d-09a5-47a2-ad92-938e0a8f0d73",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"some_python_code_for_l4_q1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9830e45-5633-4eb3-9270-643a27e24f2a",
      "metadata": {},
      "source": [
        "## Query 2: (Agentic): `Search for the latest Red Hat OpenShift version on the Red Hat website. Summarize the version number and draft a short email to my team.`\n",
        "\n",
        "Previously, we instantiated our llama-stack client with our tavily search API key, and connected to our instance of that llama-stack client, before ensuring our required tools and toolgroups are registered to that llamastack instance.\n",
        "\n",
        "Now we can create our agent, and start our agent sessions. As described above, we will need to decide if we want to use prompt-chaining or not. While its not required for this medium complexity query, as the model is good enough to do both the tool call and inference in one step, this may not be the case for other more complex queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe55883a-6887-43dd-9498-5333a51799e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
            ",\u001b[32mtool_execution> Tool:brave_search Args:{'query': 'latest Red Hat OpenShift version'}\u001b[0m\n",
            ",\u001b[32mtool_execution> Tool:brave_search Response:{\"query\": \"latest Red Hat OpenShift version\", \"top_k\": [{\"title\": \"Red Hat Enhances Security and Virtualization Experience with Latest ...\", \"url\": \"https://www.redhat.com/en/about/press-releases/red-hat-enhances-security-and-virtualization-experience-latest-version-red-hat-openshift\", \"content\": \"Red Hat, Inc., the world's leading provider of open source solutions, today announced the general availability of Red Hat OpenShift 4.18, the latest version of the industry\\u2019s leading hybrid cloud application platform powered by Kubernetes. Red Hat OpenShift 4.18 introduces new features and capabilities designed to streamline operations and security across IT environments and deliver greater consistency to all applications, from cloud-native and AI-enabled to virtualized and traditional. Additionally, for users looking for virtualization in the public cloud, Red Hat OpenShift Virtualization is now available on Oracle Cloud Infrastructure as a technology preview. Red Hat OpenShift 4.18 is now available, introducing new features and capabilities designed to streamline operations and security across IT environments and deliver greater consistency to all applications, from cloud-native and AI-enabled to virtualized and traditional.\", \"score\": 0.9296516, \"raw_content\": null}, {\"title\": \"Red Hat OpenShift 4.17: What you need to know\", \"url\": \"https://www.redhat.com/en/blog/what-you-need-to-know-red-hat-openshift-417\", \"content\": \"Based on\\u00a0Kubernetes 1.30 and\\u00a0CRI-O 1.30,\\u00a0 OpenShift 4.17 features expanded control plane options, increased flexibility for virtualization and networking, new capabilities to leverage generative AI, and continued investment in\\u00a0Red Hat OpenShift Platform Plus. Red Hat OpenShift Platform Plus (OPP) is an expanded version of Red Hat OpenShift that provides a comprehensive hybrid cloud platform with built-in security features for enterprises to build, deploy, run, manage, and automate applications at scale. For\\u00a0Red Hat OpenShift Data Foundation (ODF), we\\u2019re promoting to general availability the use of customer-managed ODF with\\u00a0Red Hat OpenShift Service on AWS with hosted control planes clusters in ODF 4.17.z. Ju Lim works on the core Red Hat OpenShift Container Platform for hybrid and multi-cloud environments to enable customers to run Red Hat OpenShift anywhere.\", \"score\": 0.8232293, \"raw_content\": null}, {\"title\": \"What's new in Red Hat OpenShift\", \"url\": \"https://www.redhat.com/en/whats-new-red-hat-openshift\", \"content\": \"What's new in Red Hat OpenShift All Red Hat Red Hat AI portfolioTune small language models and develop and deploy solutions across the hybrid cloud with our line of AI products and services. Red Hat OpenShift AI Artificial intelligenceBuild, deploy, and monitor AI models and apps with Red Hat's open source platforms. Red Hat OpenShift Service on AWS Buy onlineBuy select products and services in the Red Hat Store. Containers, Kubernetes and Red Hat OpenShift Technical Overview (No cost) Why build a Red Hat cloud? Explore Red Hat All Red Hat products Red Hat resources What's new in Red Hat OpenShift Red Hat OpenShift Red Hat Ansible Automation Platform About Red Hat About Red Hat Contact Red Hat\", \"score\": 0.48396602, \"raw_content\": null}, {\"title\": \"Red Hat OpenShift Container Platform Life Cycle Policy\", \"url\": \"https://access.redhat.com/support/policy/updates/openshift/\", \"content\": \"Extended Update Support - Long Life Add-on - Additional Term 1 enables customers to remain with the same minor release of Red Hat OpenShift for a total 24 months, allowing for stable production environments for mission-critical applications. Extended Update Support - Long Life Add-on - Additional Term 1 is provided with all Premium subscriptions for x86-64 versions of Red Hat OpenShift Kubernetes Engine, Red Hat OpenShift Container Platform, and Red Hat OpenShift Platform Plus. The combination of Extended Update Support - Long Life Add-On - Additional Term 1 and Extended Update Support Long Life Add-on - Additional Term 2 (footnote [13]) enables customers to remain with the same minor release of Red Hat OpenShift for a total of 36 months, allowing for stable production environments for mission-critical applications.\", \"score\": 0.46935064, \"raw_content\": null}, {\"title\": \"Download Red Hat Openshift - Red Hat Developer\", \"url\": \"https://developers.redhat.com/products/openshift/download\", \"content\": \"Red Hat OpenShift The trial includes membership in the Red Hat Developer program, which gives you access to evaluation software from Red Hat, tutorials, labs, cheat sheets, e-books, and more. Install Red Hat OpenShift on any supported public cloud providers: Amazon Web services, IBM, and Microsoft Azure. Create a minimal cluster on your desktop or laptop for local development and testing with Red Hat OpenShift Local. Start, stop, and deploy to Red Hat server and runtime products like Wildfly, JBoss EAP (Enterprise Application Platform), Minishift, CDK (Container Development Kit). The CLI that helps developers iterate their code on Red Hat OpenShift and Kubernetes. Red Hat OpenShift Serverless Operator Red Hat OpenShift Service Mesh Operator RED HAT DEVELOPER\", \"score\": 0.39201146, \"raw_content\": null}]}\u001b[0m\n",
            ",\u001b[33minference> \u001b[0m\u001b[33mThe\u001b[0m\u001b[33m latest\u001b[0m\u001b[33m version\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m is\u001b[0m\u001b[33m \u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m18\u001b[0m\u001b[33m.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m version\u001b[0m\u001b[33m introduces\u001b[0m\u001b[33m new\u001b[0m\u001b[33m features\u001b[0m\u001b[33m and\u001b[0m\u001b[33m capabilities\u001b[0m\u001b[33m designed\u001b[0m\u001b[33m to\u001b[0m\u001b[33m streamline\u001b[0m\u001b[33m operations\u001b[0m\u001b[33m and\u001b[0m\u001b[33m security\u001b[0m\u001b[33m across\u001b[0m\u001b[33m IT\u001b[0m\u001b[33m environments\u001b[0m\u001b[33m and\u001b[0m\u001b[33m deliver\u001b[0m\u001b[33m greater\u001b[0m\u001b[33m consistency\u001b[0m\u001b[33m to\u001b[0m\u001b[33m all\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m from\u001b[0m\u001b[33m cloud\u001b[0m\u001b[33m-native\u001b[0m\u001b[33m and\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m-enabled\u001b[0m\u001b[33m to\u001b[0m\u001b[33m virtual\u001b[0m\u001b[33mized\u001b[0m\u001b[33m and\u001b[0m\u001b[33m traditional\u001b[0m\u001b[33m.\n",
            ",\n",
            ",\u001b[0m\u001b[33mHere\u001b[0m\u001b[33m's\u001b[0m\u001b[33m a\u001b[0m\u001b[33m draft\u001b[0m\u001b[33m email\u001b[0m\u001b[33m to\u001b[0m\u001b[33m convey\u001b[0m\u001b[33m this\u001b[0m\u001b[33m information\u001b[0m\u001b[33m:\n",
            ",\n",
            ",\u001b[0m\u001b[33mSubject\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Latest\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m Version\u001b[0m\u001b[33m Available\u001b[0m\u001b[33m\n",
            ",\n",
            ",\u001b[0m\u001b[33mDear\u001b[0m\u001b[33m Team\u001b[0m\u001b[33m,\n",
            ",\n",
            ",\u001b[0m\u001b[33mI\u001b[0m\u001b[33m wanted\u001b[0m\u001b[33m to\u001b[0m\u001b[33m let\u001b[0m\u001b[33m you\u001b[0m\u001b[33m know\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m latest\u001b[0m\u001b[33m version\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m,\u001b[0m\u001b[33m version\u001b[0m\u001b[33m \u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m18\u001b[0m\u001b[33m,\u001b[0m\u001b[33m is\u001b[0m\u001b[33m now\u001b[0m\u001b[33m available\u001b[0m\u001b[33m.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m version\u001b[0m\u001b[33m includes\u001b[0m\u001b[33m new\u001b[0m\u001b[33m features\u001b[0m\u001b[33m and\u001b[0m\u001b[33m capabilities\u001b[0m\u001b[33m that\u001b[0m\u001b[33m aim\u001b[0m\u001b[33m to\u001b[0m\u001b[33m improve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m overall\u001b[0m\u001b[33m experience\u001b[0m\u001b[33m of\u001b[0m\u001b[33m using\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m enhanced\u001b[0m\u001b[33m security\u001b[0m\u001b[33m and\u001b[0m\u001b[33m virtual\u001b[0m\u001b[33mization\u001b[0m\u001b[33m options\u001b[0m\u001b[33m.\n",
            ",\n",
            ",\u001b[0m\u001b[33mIf\u001b[0m\u001b[33m you\u001b[0m\u001b[33m're\u001b[0m\u001b[33m interested\u001b[0m\u001b[33m in\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m more\u001b[0m\u001b[33m about\u001b[0m\u001b[33m the\u001b[0m\u001b[33m new\u001b[0m\u001b[33m features\u001b[0m\u001b[33m and\u001b[0m\u001b[33m capabilities\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m \u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m18\u001b[0m\u001b[33m,\u001b[0m\u001b[33m I\u001b[0m\u001b[33m recommend\u001b[0m\u001b[33m checking\u001b[0m\u001b[33m out\u001b[0m\u001b[33m the\u001b[0m\u001b[33m official\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m website\u001b[0m\u001b[33m for\u001b[0m\u001b[33m more\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\n",
            ",\n",
            ",\u001b[0m\u001b[33mBest\u001b[0m\u001b[33m,\n",
            ",\u001b[0m\u001b[33m[\u001b[0m\u001b[33mYour\u001b[0m\u001b[33m Name\u001b[0m\u001b[33m]\n",
            ",\n",
            ",\u001b[0m\u001b[33mSent\u001b[0m\u001b[33m from\u001b[0m\u001b[33m my\u001b[0m\u001b[33m llama\u001b[0m\u001b[33m-stack\u001b[0m\u001b[33m agent\u001b[0m\u001b[97m\u001b[0m\n",
            ",\u001b[30m\u001b[0m"
          ]
        }
      ],
      "source": [
        "from llama_stack_client.lib.agents.agent import Agent\n",
        "\n",
        "agent = Agent(\n",
        "    client=client,\n",
        "    model=model,\n",
        "    instructions=\"\"\"You are a helpful AI assistant, responsible for helping me find and communicate information back to my team.\n",
        "    You have access to a number of tools.\n",
        "    Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
        "    When you are asked to search the web you must use a tool.\n",
        "    When signing off on emails, please be sure to include: - Sent from my llama-stack agent in the signature\n",
        "    \"\"\",\n",
        "    tools=[\"builtin::websearch\", \"mcp::openshift\"],\n",
        "    tool_config={\"tool_choice\":\"auto\"},\n",
        "    sampling_params={\n",
        "        \"max_tokens\":4096,\n",
        "        \"strategy\": {\"type\": \"greedy\"},\n",
        "    }\n",
        ")\n",
        "\n",
        "session_id = agent.create_session(session_name=\"Draft_email_with_latest_OCP_version\")\n",
        "\n",
        "prompt_chaining = os.getenv(\"USE_PROMPT_CHAINING\") # Decide if prompt should be destructured into multiple turns or \n",
        "\n",
        "if prompt_chaining and prompt_chaining is True:\n",
        "    prompts = [\n",
        "        \"\"\"Search for the web for the latest Red Hat OpenShift version on the Red Hat website.\"\"\",\n",
        "        \"\"\"Summarize the latest Red Hat OpenShift version number and any significant features, fixes, or changes that occure in this version.\"\"\",\n",
        "        \"\"\"Draft and format an email to convey this information to my team members.\"\"\"\n",
        "    ]\n",
        "    for i, prompt in enumerate(prompts):    \n",
        "        turn_response = agent.create_turn(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ],\n",
        "            session_id=session_id,\n",
        "            stream=True,\n",
        "        )\n",
        "        logger.info(f\"========= Turn: {i} =========\")\n",
        "        for log in EventLogger().log(turn_response):\n",
        "            log.print()\n",
        "else:\n",
        "    prompt = \"\"\"Search for the web for the latest Red Hat OpenShift version on the Red Hat website. Summarize the version number and draft an email to convey this information.\"\"\"\n",
        "    turn_response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\":\"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=True,\n",
        "    )\n",
        "    for log in EventLogger().log(turn_response):\n",
        "        log.print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf81fb87-19cd-4755-96d5-59628cc75daf",
      "metadata": {},
      "source": [
        "#### Output Analysis\n",
        "\n",
        "Lets step through the output to further understands whats happening in this Agentic demo.\n",
        "\n",
        "1. First the LLM sends off a tool call to the `brave_search` to lookup the latest version of Red Hat Openshift.\n",
        "2. The LLM recieves the response from the tool call, results of the search, along with the orrigional query. Each search result has:\n",
        "    - Title of the webpage\n",
        "    - Website URL\n",
        "    - Relevant content exerpt from the search result\n",
        "    - A score, quanitifying how relevant is the search result to the search query\n",
        "    - raw_content from the page\n",
        "3. Finally, this context gets passed back to the LLM for the final inference. The inference result starts my responding to my initial question with some background, and then finally drafting the email that was requested. This example was ran without prompt-chaining."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa880bbc-bf69-4777-9417-ef7b13d51785",
      "metadata": {},
      "source": [
        "## Query 3: (MCP) `Review OpenShift logs for pods pod-123 and pod-456. Categorize each as ‘Normal’ or ‘Error’. If any show ‘Error’, send a Slack message to the ops team. Otherwise, show a simple summary.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7957e4-581c-4c3d-aee7-b8e3d9f2d0c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"some_python_code_for_l4_q3\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
