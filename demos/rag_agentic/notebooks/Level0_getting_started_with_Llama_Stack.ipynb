{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5ac225-ad97-4cbd-8e90-f0ad64cbf00b",
   "metadata": {},
   "source": [
    "# Level 0: Getting Started with Llama Stack\n",
    "\n",
    "This notebook will help you set up your environment for this tutorial. Specifically, we will cover installing the necessary libraries, configuring essential parameters, and connecting to a Llama Stack server.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "Ensure you have access to a [Llama Stack](https://llama-stack.readthedocs.io/en/latest/) server.\n",
    "\n",
    "If you need to set one up, please follow the instruction set below that is appropriate for your environment:\n",
    "\n",
    "* [Local](../../../local_setup_guide.md) setup guide for a laptop.\n",
    "* [Remote](../../../kubernetes/llama-stack/README.md) setup guide for an OpenShift cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf06a5b-7259-4e4b-94ef-fbb577c30f9e",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "This code requires `llama-stack` and the `llama-stack-client` python packages. Let's begin by installing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0ddc8b-d6cf-4cb3-8678-7b52ee70131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589b1ae-9fd5-4715-a630-cc3cc52035e9",
   "metadata": {},
   "source": [
    "## Setting the Environment Variables\n",
    "\n",
    "Rename or copy the [`.env.example`](../../../.env.example) file to create a new file called `.env`. We've included as many reasonable defaults as possible to get you started, but please use this file to make any customizations needed for your environment such as the the location of the Llama Stack server endpoint or your personal [Tavily](https://app.tavily.com) api key for web search.  \n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "### Environment variables required for all demos\n",
    "- `REMOTE_BASE_URL`: the URL of the remote Llama Stack server.\n",
    "- `INFERENCE_MODEL_ID`: the ID of the model to use for inference. The model must be available on your server.\n",
    "- `TEMPERATURE` (optional): the temperature to use during inference. Defaults to 0.0.\n",
    "- `TOP_P` (optional): the top_p parameter to use during inference. Defaults to 0.95.\n",
    "- `MAX_TOKENS` (optional): the maximum number of tokens that can be generated in the completion. Defaults to 4096.\n",
    "- `STREAM` (optional): set this to True to stream the output of the model/agent and False otherwise. Defaults to False.\n",
    "- `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Milvus Lite which is our preferred solution.\n",
    "- `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    "- `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    "- `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573899bf-34cb-4f31-9fc1-48aca439fc60",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b09511e-f6dc-4a54-b037-d64adbeaa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb65ed-f368-4bb5-957b-6698fe85b829",
   "metadata": {},
   "source": [
    "## Setting Up the Server Connection\n",
    "\n",
    "Establish the connection to your Llama Stack server.\n",
    "\n",
    "_Note: A Tavily search API key is required for some of our demos and must be provided to the client upon initialization. If you do not have one, you can set one up for free at https://app.tavily.com_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60fb3f3-4d04-4916-84fc-f798b059ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n"
     ]
    }
   ],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://localhost:8321\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f5ae8-8083-4896-a973-f310df129ec6",
   "metadata": {},
   "source": [
    "## Initializing the Inference Parameters\n",
    "\n",
    "Fetch the inference-related parameters from the corresponding environment variables and convert them to the format Llama Stack expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0092359-a5db-4d9a-a735-bb931ba05f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Parameters:\n",
      "\tModel: ibm-granite/granite-3.2-8b-instruct\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: True\n"
     ]
    }
   ],
   "source": [
    "# model_id will later be used to pass the name of the desired inference model to Llama Stack Agents/Inference APIs\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0a375",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "Now that we've set up our Tutorial environment, Let's get started building with Llama Stack! The next notebook will teach you how to build a [Simple RAG](./Level1_foundational_RAG.ipynb) application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
