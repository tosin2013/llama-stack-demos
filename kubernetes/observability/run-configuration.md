## Generate telemetry from Llamastack and vLLM

This assumes you have an observability stack running in OpenShift. To deploy the necessary components,
follow the [observability-hub guide](./README.md).

### vLLM

#### metrics

For vLLM, metrics are generated by default and are exposed at `vllm-endpoint:port/metrics`. For a list of metrics,
you can `curl localhost:8000/metrics` from within a vLLM container.

#### traces

It's possible to generate vLLM distributed trace data by updating the vLLM image and start command. This [Containerfile](./vllm-Containerfile)
shows the necessary packages to generate vLLM traces. In the future, these packages may be added to the default vLLM image available from
Red Hat OpenShift AI.

Here is how you would build vLLM with the tracing packages:

```bash
podman build --platform x86_64 -t quay.io/<your-quay-username>/vllm:otlp-tracing -f vllm-Containerfile .
podman push quay.io/[your-quay-username]/vllm:otlp-tracing
```

Then, add the following updates to the vLLM deployment.yaml. We'll use the [granite-8b deployment](../llama-serve/granite-8b/vllm.yaml):
This example assumes there is an OpenTelemetryCollector with sidecar mode in the same namespace.
See [OpenTelemetryCollector Sidecars Deployment](./README.md#opentelemetrycollector_sidecars_deployment)


```yaml
---
  template:
    metadata:
      labels:
        app: granite-8b
      annotations:
        sidecar.opentelemetry.io/inject: vllm-otelsidecar
    spec:
      containers:
      - args:
        - --model
        - ibm-granite/granite-3.2-8b-instruct
        - --max-model-len
        - "128000"
        - --enable-auto-tool-choice
        - --chat-template
        - /app/tool_chat_template_granite.jinja
        - --tool-call-parser=granite
        - --otlp-traces-endpoint
        - 127.0.0.1:4317
        - --collect-detailed-traces
        - "all"
        - --port
        - "8000"
        image: 'quay.io/<your-quay-username>/vllm:otlp-tracing'
        env:
        - name: OTEL_SERVICE_NAME
          value: "vllm-granite8b"
        - name: OTEL_EXPORTER_OTLP_TRACES_INSECURE
          value: "true"
---
```

With the updated vLLM image and the updated deployment, distributed trace data will be generated and collected by the opentelemetry-collector
sidecar container and exported to the central observability-hub as outlined in the [README.md](./README.md) with a `TempoStack` as a tracing backend.

There is a performance impact with enabling tracing, so it's recommended to update the deployment to enable tracing only when debugging to
avoid the performance impact. A complete list of vLLM engine arguments can be found [here](https://docs.vllm.ai/en/latest/serving/engine_args.html).

### Llamastack

With Llamastack, update the [configmap](../llama-stack/configmap.yaml) to enable telemetry collection with an opentelemetry receiver.
Don't update these until _after_ the [OpentelemetryCollector Sidecar](./otel-collector/otel-collector-llamastack-sidecar.yaml)
is deployed. Follow the [observability-hub guide](./README.md)
to install the `RH Build of OpenTelemetry Operator` and `OpenTelemetryCollector`.

#### Updated manifests for telemetry trace collection with opentelemetry receiver endpoint

This is for traces only. There is a similar `otel_metric` sink and `otel_metric_endpoint`, however, there are currently
only 4 metrics generated within Llamastack, and these are duplicates of what vLLM provides.

[kubernetes/llama-stack/configmap.yaml](../llama-stack/configmap.yaml)

```yaml
---
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: ${env.OTEL_SERVICE_NAME:llama-stack}
          sinks: ${env.TELEMETRY_SINKS:console, otel_trace, sqlite} <-add otel_trace and/or otel_metric
          otel_trace_endpoint: ${env.OTEL_TRACE_ENDPOINT:} <-add ONLY if opentelemetry receiver endpoint is available.
---
```
And, in [kubernetes/llama-stack/deployment.yaml](../llama-stack/deployment.yaml)

```yaml
---
  template:
    metadata:
      labels:
        app: llama-stack
      annotations:
        sidecar.opentelemetry.io/inject: llamastack-otelsidecar
    spec:
      containers:
---
        env:
        - name: OTEL_SERVICE_NAME
          value: llamastack
        - name: OTEL_TRACE_ENDPOINT
          value: http://localhost:4318/v1/traces
       #-  name: OTEL_METRIC_ENDPOINT
       #-  value: http://localhost:4318/v1/metrics
---
```

If _not_ using an opentelemetry-collector sidecar, you can send to any in-cluster opentelemetry-collector by setting the
`OTEL_TRACE_ENDPOINT` to `http://service-name-otc.namespace-of-otc.svc.cluster.local:4318/v1/traces,metrics`. The sidecar
annotation is only required if using an opentelemetry-collector sidecar.

Don't update the Llamastack deployment until _after_ the [OpentelemetryCollector Sidecar](./otel-collector/otel-collector-llamastack-sidecar.yaml)
and/or the [central OpenTelemetryCollector](./otel-collector/otel-collector.yaml) is deployed.
