# Workshop Template System Configuration
# This file contains all customizable parameters for easy deployment

# System Configuration
system:
  name: "Workshop Template System"
  version: "1.0.0"
  environment: "production"

# URL Configuration - Easily customizable for different environments
urls:
  # Gitea server URL - customize for your Gitea deployment
  gitea_url: "https://gitea.apps.cluster.local"
  
  # Workshop domain - customize for your OpenShift cluster
  workshop_domain: "apps.cluster.local"
  
  # Llama Stack endpoint - internal service URL
  llama_stack_endpoint: "http://llama-stack-server:8321"

# Workshop Configuration
workshops:
  healthcare_ml:
    name: "Healthcare ML Genetic Predictor"
    repository: "https://github.com/tosin2013/healthcare-ml-genetic-predictor.git"
    gitea_repo: "workshop-system/healthcare-ml-workshop.git"
    type: "application-conversion"
    technologies: ["quarkus", "kafka", "openshift", "ml"]
    subdomain: "healthcare-ml"
    
  openshift_baremetal:
    name: "OpenShift Bare Metal Deployment - Enhanced"
    repository: "https://github.com/Red-Hat-SE-RTO/openshift-bare-metal-deployment-workshop.git"
    gitea_repo: "workshop-system/openshift-baremetal-workshop.git"
    type: "existing-workshop-enhancement"
    technologies: ["openshift", "bare-metal", "installation"]
    subdomain: "openshift-baremetal"

# Agent Configuration
agents:
  workshop_chat:
    port: 8080
    replicas: 2
    rag_enabled: true
    memory_request: "1Gi"
    memory_limit: "2Gi"
    cpu_request: "500m"
    cpu_limit: "1"
    
  template_converter:
    port: 8080
    replicas: 1
    analysis_depth: "standard"
    memory_request: "1Gi"
    memory_limit: "2Gi"
    cpu_request: "500m"
    cpu_limit: "1"
    
  content_creator:
    port: 8080
    replicas: 1
    showroom_integration: true
    memory_request: "1Gi"
    memory_limit: "2Gi"
    cpu_request: "500m"
    cpu_limit: "1"
    
  source_manager:
    port: 8080
    replicas: 1
    deployment_targets: ["openshift", "github-pages"]
    memory_request: "1Gi"
    memory_limit: "2Gi"
    cpu_request: "500m"
    cpu_limit: "1"
    
  research_validation:
    port: 8080
    replicas: 1
    web_search_enabled: true
    memory_request: "1Gi"
    memory_limit: "2Gi"
    cpu_request: "500m"
    cpu_limit: "1"
    
  documentation_pipeline:
    port: 8080
    replicas: 1
    monitoring_frequency: "daily"
    memory_request: "1Gi"
    memory_limit: "2Gi"
    cpu_request: "500m"
    cpu_limit: "1"

# Llama Stack Configuration
llama_stack:
  port: 8321
  model_id: "meta-llama/Llama-3.2-3B-Instruct"
  memory_request: "4Gi"
  memory_limit: "8Gi"
  cpu_request: "2"
  cpu_limit: "4"
  storage_size: "10Gi"

# Build Configuration
build:
  base_image: "registry.access.redhat.com/ubi8/httpd-24:latest"
  webhook_secret: "workshop-webhook-secret"
  
# Security Configuration
security:
  service_account: "workshop-system-sa"
  
# Monitoring Configuration
monitoring:
  enabled: true
  health_check_interval: "30s"
  readiness_timeout: "10s"
