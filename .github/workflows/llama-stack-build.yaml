name: Llama stack builds
on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Llama stack release (e.g. 0.1.8, 0.1.9)'
        required: true
        type: string
        default: '0.1.9'


jobs:
  llama-stack-build:
    runs-on: ubuntu-latest
    environment: ci
    steps:
      - name: Checkout release tag
        uses: actions/checkout@v4
        with:
           ref: ${{ inputs.version }}
           repository: meta-llama/llama-stack.git

      - name: python deps
        run: |
           sudo apt-get update
           sudo apt-get install -y python3-pip

      - name: login to quay.io
        uses: docker/login-action@v3
        with:
          registry: quay.io
          username: ${{ secrets.QUAY_USERNAME }}
          password: ${{ secrets.QUAY_PASSWORD }}

      - name: Install repo packages
        run: |
          pip install -U .

      - name: modify the build.yaml
        run: |
          sed -i 's/    - inline::faiss/    - inline::milvus/g' llama_stack/templates/remote-vllm/build.yaml
          sed -i '/^ *- inline::faiss$/d; /^ *- remote::chromadb$/d; /^ *- remote::pgvector$/d' llama_stack/templates/remote-vllm/build.yaml
          sed -i 's/image_type: conda/image_type: container/g' llama_stack/templates/remote-vllm/build.yaml

      - name: display contents of buld.yaml
        run: |
          cat llama_stack/templates/remote-vllm/build.yaml

      - name: run llama stack build
        run: |
          export CONTAINER_BINARY=podman
          llama stack build --config llama_stack/templates/remote-vllm/build.yaml --image-type container --image-name remote-vllm

      - name: tag the image
        run: |
          podman tag localhost/remote-vllm:${{inputs.version}} quay.io/redhat-et/llama:vllm-${{inputs.version}}

      - name: push the image
        run: |
          podman push quay.io/redhat-et/llama:vllm-${{inputs.version}}
