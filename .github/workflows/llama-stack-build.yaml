name: Llama stack builds
on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Llama stack release (e.g. 0.1.8, 0.1.9)'
        required: true
        type: string
        default: '0.1.9'
      external_provider:
        description: 'Enable external providers directory'
        required: false
        type: boolean
        default: true

jobs:
  llama-stack-build:
    runs-on: ubuntu-latest
    environment: ci
    steps:
      - name: Checkout release tag
        uses: actions/checkout@v4
        with:
           ref: v${{ inputs.version }}
           repository: meta-llama/llama-stack.git

      - name: python deps
        run: |
           sudo apt-get update
           sudo apt-get install -y python3-pip

      - name: login to quay.io
        uses: docker/login-action@v3
        with:
          registry: quay.io
          username: ${{ secrets.QUAY_USERNAME }}
          password: ${{ secrets.QUAY_PASSWORD }}

      - name: Install repo packages
        run: |
          pip install -U .
          cd ..

      - name: clone the llama stack demo repo
        uses: actions/checkout@v4
        with:
          repository: opendatahub-io/llama-stack-demos.git
          ref: main

      - name: run llama stack build
        if: ${{ inputs.external_provider == false }}
        run: |
          cd distribution
          export CONTAINER_BINARY=podman
          mkdir -p /home/runner/.llama/providers.d
          llama stack build --config build.yaml

      - name: run llama stack build(with external)
        if: ${{ inputs.external_provider == true }}
        run: |
          cd distribution
          export CONTAINER_BINARY=podman
          llama stack build --config build-external-provider.yaml

      - name: tag the image (with external)
        if: ${{ inputs.external_provider == true }}
        run: |
          podman tag localhost/llama-stack-demos:${{inputs.version}} quay.io/redhat-et/llama:vllm-${{inputs.version}}-external

      - name: push the image (with external)
        if: ${{ inputs.external_provider == true }}
        run: |
          podman push quay.io/redhat-et/llama:vllm-${{inputs.version}}-external

      - name: tag the image (without external)
        if: ${{ inputs.external_provider == false }}
        run: |
          podman tag localhost/llama-stack-demos:${{inputs.version}} quay.io/redhat-et/llama:vllm-${{inputs.version}}

      - name: push the image (without external)
        if: ${{ inputs.external_provider == false }}
        run: |
          podman push quay.io/redhat-et/llama:vllm-${{inputs.version}}

  granite-llama-stack-build:
    runs-on: ubuntu-latest
    environment: ci
    steps:
      - name: Checkout release tag
        uses: actions/checkout@v4
        with:
           ref: v${{ inputs.version }}
           repository: meta-llama/llama-stack.git

      - name: python deps
        run: |
           sudo apt-get update
           sudo apt-get install -y python3-pip

      - name: login to quay.io
        uses: docker/login-action@v3
        with:
          registry: quay.io
          username: ${{ secrets.QUAY_USERNAME }}
          password: ${{ secrets.QUAY_PASSWORD }}

      - name: Install repo packages
        run: |
          pip install -U .
          cd ..

      - name: clone the llama stack demo repo
        uses: actions/checkout@v4
        with:
          repository: opendatahub-io/llama-stack-demos.git
          ref: main

      - name: run llama stack build
        run: |
          cd distribution/remote-vllm-granite-embedding
          export CONTAINER_BINARY=podman
          mkdir -p /home/runner/.llama/providers.d
          llama stack build --config build.yaml

      - name: tag the image
        run: |
          podman tag localhost/llama-stack-demos:${{inputs.version}} quay.io/redhat-et/llama:vllm-milvus-granite-${{inputs.version}}

      - name: push the image
        run: |
          podman push quay.io/redhat-et/llama:vllm-milvus-granite-${{inputs.version}}
